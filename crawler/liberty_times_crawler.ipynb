{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe that we can get news of a day with the link format:\n",
    "http://news.ltn.com.tw/list/newspaper/politics/20181231\n",
    "#### We need a list of date string in the date range we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = \"2018-07-01\" #new_test\n",
    "stop_date = \"2018-12-31\"\n",
    "\n",
    "start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "stop = datetime.strptime(stop_date, \"%Y-%m-%d\")\n",
    "\n",
    "dates = list()\n",
    "while start <= stop:\n",
    "    dates.append(start.strftime('%Y%m%d'))\n",
    "    start = start + timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now write a function to parse the HTML response, return the data we want(title, content, ...etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document, date):\n",
    "    \n",
    "    nodes = document.select('ul.list > li')\n",
    "    data = list()\n",
    "\n",
    "    for li in nodes:\n",
    "\n",
    "        # check if is empty element\n",
    "        if li.select_one('a') == None:\n",
    "            continue\n",
    "\n",
    "        # get link\n",
    "        li_link = 'http://news.ltn.com.tw/' + li.select_one('a')['href']\n",
    "\n",
    "        # request for document\n",
    "        li_res = requests.get(li_link)\n",
    "        li_doc = bs(li_res.text, 'lxml')\n",
    "\n",
    "        # get date\n",
    "        li_date = datetime.strptime(date, \"%Y%m%d\").strftime('%Y-%m-%d')\n",
    "\n",
    "        #get title\n",
    "        li_title = li.select_one('p').get_text()\n",
    "\n",
    "        #get content\n",
    "        li_content = \"\"\n",
    "        for ele in li_doc.select('div.text > p'):\n",
    "            if not 'appE1121' in ele.get('class', []):\n",
    "                li_content += ele.get_text()\n",
    "\n",
    "        # append new row\n",
    "        data.append({\n",
    "            'date' : li_date,\n",
    "            'title': li_title,\n",
    "            'link' : li_link,\n",
    "            'content' : li_content,\n",
    "            'tags' : []\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl over the news on the site, store the data in variable \"all_data\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start crawling : 20180701\n",
      "start crawling : 20180702\n",
      "start crawling : 20180703\n",
      "start crawling : 20180704\n",
      "start crawling : 20180705\n",
      "start crawling : 20180706\n",
      "start crawling : 20180707\n",
      "start crawling : 20180708\n",
      "start crawling : 20180709\n",
      "start crawling : 20180710\n",
      "start crawling : 20180711\n",
      "start crawling : 20180712\n",
      "start crawling : 20180713\n",
      "start crawling : 20180714\n",
      "start crawling : 20180715\n",
      "start crawling : 20180716\n",
      "start crawling : 20180717\n",
      "start crawling : 20180718\n",
      "start crawling : 20180719\n",
      "start crawling : 20180720\n",
      "start crawling : 20180721\n",
      "start crawling : 20180722\n",
      "start crawling : 20180723\n",
      "start crawling : 20180724\n",
      "start crawling : 20180725\n",
      "start crawling : 20180726\n",
      "start crawling : 20180727\n",
      "start crawling : 20180728\n",
      "start crawling : 20180729\n",
      "start crawling : 20180730\n",
      "start crawling : 20180731\n",
      "start crawling : 20180801\n",
      "start crawling : 20180802\n",
      "start crawling : 20180803\n",
      "start crawling : 20180804\n",
      "start crawling : 20180805\n",
      "start crawling : 20180806\n",
      "start crawling : 20180807\n",
      "start crawling : 20180808\n",
      "start crawling : 20180809\n",
      "start crawling : 20180810\n",
      "start crawling : 20180811\n",
      "start crawling : 20180812\n",
      "start crawling : 20180813\n",
      "start crawling : 20180814\n",
      "start crawling : 20180815\n",
      "start crawling : 20180816\n",
      "start crawling : 20180817\n",
      "start crawling : 20180818\n",
      "start crawling : 20180819\n",
      "start crawling : 20180820\n",
      "start crawling : 20180821\n",
      "start crawling : 20180822\n",
      "start crawling : 20180823\n",
      "start crawling : 20180824\n",
      "start crawling : 20180825\n",
      "start crawling : 20180826\n",
      "start crawling : 20180827\n",
      "start crawling : 20180828\n",
      "start crawling : 20180829\n",
      "start crawling : 20180830\n",
      "start crawling : 20180831\n",
      "start crawling : 20180901\n",
      "start crawling : 20180902\n",
      "start crawling : 20180903\n",
      "start crawling : 20180904\n",
      "start crawling : 20180905\n",
      "start crawling : 20180906\n",
      "start crawling : 20180907\n",
      "start crawling : 20180908\n",
      "start crawling : 20180909\n",
      "start crawling : 20180910\n",
      "start crawling : 20180911\n",
      "start crawling : 20180912\n",
      "start crawling : 20180913\n",
      "start crawling : 20180914\n",
      "start crawling : 20180915\n",
      "start crawling : 20180916\n",
      "start crawling : 20180917\n",
      "start crawling : 20180918\n",
      "start crawling : 20180919\n",
      "start crawling : 20180920\n",
      "start crawling : 20180921\n",
      "start crawling : 20180922\n",
      "start crawling : 20180923\n",
      "start crawling : 20180924\n",
      "start crawling : 20180925\n",
      "start crawling : 20180926\n",
      "start crawling : 20180927\n",
      "start crawling : 20180928\n",
      "start crawling : 20180929\n",
      "start crawling : 20180930\n",
      "start crawling : 20181001\n",
      "start crawling : 20181002\n",
      "start crawling : 20181003\n",
      "start crawling : 20181004\n",
      "start crawling : 20181005\n",
      "start crawling : 20181006\n",
      "start crawling : 20181007\n",
      "start crawling : 20181008\n",
      "start crawling : 20181009\n",
      "start crawling : 20181010\n",
      "start crawling : 20181011\n",
      "start crawling : 20181012\n",
      "start crawling : 20181013\n",
      "start crawling : 20181014\n",
      "start crawling : 20181015\n",
      "start crawling : 20181016\n",
      "start crawling : 20181017\n",
      "start crawling : 20181018\n",
      "start crawling : 20181019\n",
      "start crawling : 20181020\n",
      "start crawling : 20181021\n",
      "start crawling : 20181022\n",
      "start crawling : 20181023\n",
      "start crawling : 20181024\n",
      "start crawling : 20181025\n",
      "start crawling : 20181026\n",
      "start crawling : 20181027\n",
      "start crawling : 20181028\n",
      "start crawling : 20181029\n",
      "start crawling : 20181030\n",
      "start crawling : 20181031\n",
      "start crawling : 20181101\n",
      "start crawling : 20181102\n",
      "start crawling : 20181103\n",
      "start crawling : 20181104\n",
      "start crawling : 20181105\n",
      "start crawling : 20181106\n",
      "start crawling : 20181107\n",
      "start crawling : 20181108\n",
      "start crawling : 20181109\n",
      "start crawling : 20181110\n",
      "start crawling : 20181111\n",
      "start crawling : 20181112\n",
      "start crawling : 20181113\n",
      "start crawling : 20181114\n",
      "start crawling : 20181115\n",
      "start crawling : 20181116\n",
      "start crawling : 20181117\n",
      "start crawling : 20181118\n",
      "start crawling : 20181119\n",
      "start crawling : 20181120\n",
      "start crawling : 20181121\n",
      "start crawling : 20181122\n",
      "start crawling : 20181123\n",
      "start crawling : 20181124\n",
      "start crawling : 20181125\n",
      "start crawling : 20181126\n",
      "start crawling : 20181127\n",
      "start crawling : 20181128\n",
      "start crawling : 20181129\n",
      "start crawling : 20181130\n",
      "start crawling : 20181201\n",
      "start crawling : 20181202\n",
      "start crawling : 20181203\n",
      "start crawling : 20181204\n",
      "start crawling : 20181205\n",
      "start crawling : 20181206\n",
      "start crawling : 20181207\n",
      "start crawling : 20181208\n",
      "start crawling : 20181209\n",
      "start crawling : 20181210\n",
      "start crawling : 20181211\n",
      "start crawling : 20181212\n",
      "start crawling : 20181213\n",
      "start crawling : 20181214\n",
      "start crawling : 20181215\n",
      "start crawling : 20181216\n",
      "start crawling : 20181217\n",
      "start crawling : 20181218\n",
      "start crawling : 20181219\n",
      "start crawling : 20181220\n",
      "start crawling : 20181221\n",
      "start crawling : 20181222\n",
      "start crawling : 20181223\n",
      "start crawling : 20181224\n",
      "start crawling : 20181225\n",
      "start crawling : 20181226\n",
      "start crawling : 20181227\n",
      "start crawling : 20181228\n",
      "start crawling : 20181229\n",
      "start crawling : 20181230\n",
      "start crawling : 20181231\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "all_data = list()\n",
    "for date in dates:\n",
    "    print('start crawling :', date)\n",
    "    res = requests.get('https://news.ltn.com.tw/list/newspaper/politics/' + date) #到自由時報的政治版抓內容，並透過修改yyyymmdd來抓不同日期\n",
    "    doc = bs(res.text, 'lxml') #整里程可由電腦閱讀之樣式dict?\n",
    "    data = process_document(doc, date)\n",
    "    all_data += data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/liberty_times.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn it into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>新南向救災合作 推動消防設備輸出</td>\n",
       "      <td>http://news.ltn.com.tw/news/politics/paper/121...</td>\n",
       "      <td>〔記者李欣芳／台北報導〕政府積極推動新南向政策，其中推動與新南向國家的救災合作已有腹案，行政...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>竹山訓練中心 亞洲最大</td>\n",
       "      <td>http://news.ltn.com.tw/news/politics/paper/121...</td>\n",
       "      <td>〔記者陳薏云／新北報導〕位於南投竹山的消防署訓練中心於二○一○年一月十九日啟用，是目前亞洲規...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>9月正式開幕 我盼美高階官員來台 見證AIT新館</td>\n",
       "      <td>http://news.ltn.com.tw/news/politics/paper/121...</td>\n",
       "      <td>〔記者蘇永耀、彭琬馨／台北報導〕美國在台協會（ＡＩＴ）內湖新館已於六月落成，正式運作將至九月...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>陳破空︰美中對決 台應掌握戰略機遇</td>\n",
       "      <td>http://news.ltn.com.tw/news/politics/paper/121...</td>\n",
       "      <td>〔記者呂伊萱／台北報導〕美國總統川普上任後，以非典型領導與強勢多變的談判作風「讓美國再次強大...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>冷眼集》大大同情中國當局</td>\n",
       "      <td>http://news.ltn.com.tw/news/politics/paper/121...</td>\n",
       "      <td>記者鄒景雯／特稿最近有位被拒絕入境的中國記者，對於無法再回到台灣駐點的遭遇，表達了「有一點點...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                     title  \\\n",
       "0  2018-07-01          新南向救災合作 推動消防設備輸出   \n",
       "1  2018-07-01               竹山訓練中心 亞洲最大   \n",
       "2  2018-07-01  9月正式開幕 我盼美高階官員來台 見證AIT新館   \n",
       "3  2018-07-01         陳破空︰美中對決 台應掌握戰略機遇   \n",
       "4  2018-07-01              冷眼集》大大同情中國當局   \n",
       "\n",
       "                                                link  \\\n",
       "0  http://news.ltn.com.tw/news/politics/paper/121...   \n",
       "1  http://news.ltn.com.tw/news/politics/paper/121...   \n",
       "2  http://news.ltn.com.tw/news/politics/paper/121...   \n",
       "3  http://news.ltn.com.tw/news/politics/paper/121...   \n",
       "4  http://news.ltn.com.tw/news/politics/paper/121...   \n",
       "\n",
       "                                             content tags  \n",
       "0  〔記者李欣芳／台北報導〕政府積極推動新南向政策，其中推動與新南向國家的救災合作已有腹案，行政...   []  \n",
       "1  〔記者陳薏云／新北報導〕位於南投竹山的消防署訓練中心於二○一○年一月十九日啟用，是目前亞洲規...   []  \n",
       "2  〔記者蘇永耀、彭琬馨／台北報導〕美國在台協會（ＡＩＴ）內湖新館已於六月落成，正式運作將至九月...   []  \n",
       "3  〔記者呂伊萱／台北報導〕美國總統川普上任後，以非典型領導與強勢多變的談判作風「讓美國再次強大...   []  \n",
       "4  記者鄒景雯／特稿最近有位被拒絕入境的中國記者，對於無法再回到台灣駐點的遭遇，表達了「有一點點...   []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(all_data)[['date', 'title', 'link', 'content', 'tags']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
